{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nGradient enhanced gPC\n=====================\n\nIntroduction\n^^^^^^^^^^^^\nIt is possible to accelerate the identification of the gPC coefficients by using the derivative information\nof the transfer function. The gPC matrix consists of continuously differentiable polynomials and can be extended\nby its partial derivatives at each sampling point. This extends the resulting system of equations to:\n\n\\begin{align}\\left[ \\begin{array}{c}\\mathbf{\\Psi}\\\\\n    \\mathbf{\\Psi}_\\partial\\end{array}\\right][\\mathbf{U}] =\n    \\left[ \\begin{array}{c} \\mathbf{Y}\\\\\n    \\mathbf{Y}_\\partial \\end{array}\\right]\\end{align}\n\nwhere the gradient gPC matrix $[\\mathbf{\\Psi}_\\partial]$ is of size $[d N_g \\times N_c]$ and\ncontains the partial derivatives of the basis functions at each sampling point:\n\n\\begin{align}[\\mathbf{\\Psi}_\\partial] =\n    \\left[ \\begin{array}{c}\n    \\left.\\frac{\\partial\\psi}{\\partial\\xi_1}\\right|_{\\xi_1}\\\n    \\vdots\\\\\n    \\left.\\frac{\\partial\\psi}{\\partial\\xi_d}\\right|_{\\xi_1}\\\n    \\vdots\\\\\n    \\vdots\\\\\n    \\vdots\\\\\n    \\left.\\frac{\\partial\\psi}{\\partial\\xi_1}\\right|_{\\xi_{N_g}}\\\\\n    \\vdots\\\\\n    \\left.\\frac{\\partial\\psi}{\\partial\\xi_d}\\right|_{\\xi_{N_g}}\n    \\end{array}\\right]\\end{align}\n\nThe solution on the right hand side is extended accordingly:\n\n\\begin{align}[\\mathbf{Y}_\\partial] =\n    \\left[ \\begin{array}{ccc}\n    \\left.\\frac{\\partial y_1}{\\partial\\xi_1}\\right|_{\\xi_1} & \\ldots & \\left.\\frac{\\partial y_{N_q}}{\\partial\\xi_1}\\right|_{\\xi_1}\\\\\n    \\vdots & \\vdots & \\vdots\\\\\n    \\left.\\frac{\\partial y_1}{\\partial\\xi_d}\\right|_{\\xi_1} & \\ldots & \\left.\\frac{\\partial y_{N_q}}{\\partial\\xi_d}\\right|_{\\xi_1}\\\\\n    \\vdots & \\ldots & \\vdots\\\\\n    \\vdots & \\ddots & \\vdots\\\\\n    \\vdots & \\ldots & \\vdots\\\\\n    \\left.\\frac{\\partial y_1}{\\partial\\xi_1}\\right|_{\\xi_{N_g}} & \\ldots & \\left.\\frac{\\partial y_{N_q}}{\\partial\\xi_1}\\right|_{\\xi_{N_g}}\\\\\n    \\vdots & \\vdots & \\vdots\\\\\n    \\left.\\frac{\\partial y_1}{\\partial\\xi_d}\\right|_{\\xi_{N_g}} & \\ldots & \\left.\\frac{\\partial y_{N_q}}{\\partial\\xi_d}\\right|_{\\xi_{N_g}}\n    \\end{array}\\right]\\end{align}\n\nThe complete system now reads:\n\n\\begin{align}[\\mathbf{\\Psi}'][\\mathbf{U}] = [\\mathbf{Y}']\\end{align}\n\nThis gradient based formulation consists of $(d+1) N_g$ equations that match both function values and gradients,\nin comparison to traditional approaches which consists of only $N_g$ equations that match function values.\nDespite the extra computational cost required to obtain the gradients, the use of gradients improves the gPC.\nHowever, there exist several methods to determine the gradients. In the following, the implemented methods\nin pygpc are presented and compared.\n\nGradient estimation of sparse irregular datasets\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nSurface interpolation finds application in many aspects of science and technology.\nA typical application in geological science and environmental engineering is to contour\nsurfaces from hydraulic head measurements from irregular spaced data.\n\nComputing gradients efficiently and accurately from sparse irregular high-dimensional\ndatasets is challenging. The additional calculation effort should be kept as low as possible,\nespecially in the case of computationally expensive models\n\nFinite difference approach using forward approximation\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nLet $f:D \\subset \\mathbb{R}^d \\rightarrow \\mathbb{R}$ be differentiable at\n$\\mathbf{x}_0 \\in D$. Taylor's theorem for several variables states that:\n\n\\begin{align}f(\\mathbf{x}_0 + \\mathbf{h}) = f(\\mathbf{x}_0) + \\frac{(\\mathbf{h} \\cdot \\nabla)f(\\mathbf{x}_0)}{1!}\n    + ... + \\frac{(\\mathbf{h} \\cdot \\nabla)^rf(\\mathbf{x}_0)}{r!} + R_r\\end{align}\n\nwhere the remainder $R_r$ has the Lagrange form:\n\n\\begin{align}\\frac{(\\mathbf{h} \\cdot \\nabla)^{r+1} f(\\mathbf{x}_0 + \\theta \\mathbf{h})}{(r+1)!}\\end{align}\n\nTruncating the Taylor series after the first term leads to:\n\n\\begin{align}\\nabla f(\\mathbf{x}_0) = \\frac{f(\\mathbf{x}_0 + \\mathbf{h}) -\n    f(\\mathbf{x}_0)}{\\left\\lVert\\mathbf{h}\\right\\lVert}\\end{align}\n\nIn order to approximate the gradient it is necessary to determine addtitional function\nvalues $f(\\mathbf{x}_0 + \\mathbf{h})$ at small displacements $\\mathbf{h}$\nin every dimension. Each additional row in the gradient gPC matrix\n$[\\mathbf{\\Psi}_\\partial]$ thus requires one additional model evaluation.\n\n![](../../../examples/images/FD_fwd.png)\n\n    :width: 500\n    :align: center\n\nThis torpedoes the previously mentioned advantage of the gradient enhanced gPC approach in terms of efficacy.\n\nFinite difference regression approach of 1st order accuracy\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nSuppose that $\\mathbf{x}_0=(x_1, ..., x_d)^\\mathrm{T}$ is the point where we want to estimate the\ngradient and we are given $p$ scattered data points $\\mathbf{x}_i = (x_{1,i}, ..., x_{d,i}),\ni = 1, ..., p$, which are located closely to\n$\\mathbf{x}_0$ such that $\\mathbf{x}_i \\in \\mathcal{B}_\\delta(\\mathbf{x}_0)$.\n\n![](../../../examples/images/FD_1st.png)\n\n    :width: 250\n    :align: center\n\nTruncating the Taylor expansion after the first term allows to write an overdetermined\nsystem of equations for $\\mathbf{g}=\\left(\\frac{\\partial f}{\\partial x_1}, ... ,\n\\frac{\\partial f}{\\partial x_d} \\right)^\\mathrm{T}$ in the form:\n\n\\begin{align}[\\mathbf{D}] \\mathbf{g} = \\delta\\mathbf{f}\\end{align}\n\nwhose least squares solution provides a first order estimate of the gradient.\nThe matrix $\\mathbf{D}\\in\\mathbb{R}^{p \\times d}$ contains the distances between the\nsurrounding points $\\mathbf{x}_i$ and the point $\\mathbf{x}_0$ and is given by:\n\n\\begin{align}[\\mathbf{D}] =\n    \\left[ \\begin{array}{ccc}\n    (x_{1,1} - x_{1,0}) & \\ldots & (x_{d,1} - x_{d,0}) \\\\\n    \\vdots & \\ddots & \\vdots \\\\\n    (x_{1,p} - x_{1,0}) & \\ldots & (x_{d,p} - x_{d,0})\n    \\end{array}\\right]\n    =\n    \\left[ \\begin{array}{ccc}\n    \\delta x_{1,1} & \\ldots & \\delta x_{d,1} \\\\\n    \\vdots & \\ddots & \\vdots \\\\\n    \\delta x_{1,p} & \\ldots & \\delta x_{d,p}\n    \\end{array}\\right]\\end{align}\n\nThe differences of the model solutions $\\delta f_i = f(\\mathbf{x}_0 + \\delta\\mathbf{x}_i)-f(\\mathbf{x}_0)$\nare collected in the vector $\\delta \\mathbf{f} \\in \\mathbb{R}^{p \\times 1}$.\n\nEach adjacent point may be weighted by its distance to $\\mathbf{x}_0$.\nThis can be done by introducing a weight matrix $[\\mathbf{W}] =\n\\mathrm{diag}(\\left\\lvert\\delta\\mathbf{x}_1\\right\\lvert^{\\alpha}, ...,\n\\left\\lvert\\delta\\mathbf{x}_p\\right\\lvert^{\\alpha})$ with $\\alpha=-1$\nfor inverse distance or $\\alpha=-2$ for inverse distance squared.\n\n\\begin{align}[\\mathbf{W}][\\mathbf{D}] \\mathbf{g} = [\\mathbf{W}]\\delta\\mathbf{f}\\end{align}\n\nThe least squares solution of the gradient is then given by:\n\n\\begin{align}\\mathbf{g} = \\left([\\mathbf{W}][\\mathbf{D}]\\right)^+[\\mathbf{W}]\\delta\\mathbf{f}\\end{align}\n\nThis procedure has to be repeated for every sampling point $\\mathbf{x}_0$.\nWith this approach, it is possible to estimate the gradients only from the available\ndata points without the need to run additional simulations. However, one has to suitably\nchoose the values of $\\delta$ and $\\alpha$. If the sampling points are too\nfar away from each other, it may not be possible to estimate the gradient accurately.\n\nFinite difference regression approach of 2nd order accuracy\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTruncating the Taylor expansion after the second term enables the following overdetermined\nsystem to be solved, in the least squared sense, to obtain a second order approximation for the gradient:\n\n\\begin{align}[\\mathbf{D}:\\mathbf{M}]\\left[ \\begin{array}{c}\n    \\mathbf{g}\\\\\n    \\mathbf{z}\\\\\n    \\end{array}\\right]=\n    \\delta \\mathbf{f}\\end{align}\n\nwhere the second order distance matrix $[\\mathbf{M}] \\in \\mathbb{R}^{p \\times \\sum_{i=1}^{d} i}$ given by:\n\n\\begin{align}[\\mathbf{M}]=\n    \\left[\\begin{array}{cccc}\n    \\frac{1}{2}\\delta x_{1,1}^2 & \\delta x_{1,1} \\delta x_{2,1} & \\ldots & \\frac{1}{2}\\delta x_{d,1}^2\\\\\n    \\vdots & \\ldots & \\ldots & \\vdots \\\\\n    \\frac{1}{2}\\delta x_{1,p}^2 & \\delta x_{1,p} \\delta x_{2,p} & \\ldots & \\frac{1}{2}\\delta x_{d,p}^2\\\\\n    \\end{array}\\right]\\end{align}\n\nThe vector $\\mathbf{z}=\\left(\\frac{\\partial^2 f}{\\partial x_1^2},\n\\frac{\\partial^2 f}{\\partial x_1 x_2} , ..., \\frac{\\partial^2 f}{\\partial x_d^2}\\right)^\\mathrm{T}$\ncontains the second derivatives. The new system of equations can be written as:\n\n\\begin{align}[\\mathbf{D}] \\mathbf{g} = \\delta\\mathbf{f} - [\\mathbf{M}] \\mathbf{z}\\end{align}\n\nApplying the weight matrix $[\\mathbf{W}] = \\mathrm{diag}(\\left\\lvert\\delta\\mathbf{x}_1\n\\right\\lvert^{\\alpha}, ..., \\left\\lvert\\delta\\mathbf{x}_p\\right\\lvert^{\\alpha})$ leads:\n\n\\begin{align}[\\mathbf{W}][\\mathbf{D}] \\mathbf{g} = [\\mathbf{W}]\\delta\\mathbf{f} - [\\mathbf{W}][\\mathbf{M}] \\mathbf{z}\\end{align}\n\nfrom which it can be seen that a more accurate estimate of the gradient than that offered as the previous\napproach can be obtained if the second order derivative terms are eliminated from the system.\nThis elimination can be performed using QR decomposition of $[\\mathbf{W}][\\mathbf{M}]$,\nnamely $[\\mathbf{Q}]^{\\mathrm{T}}[\\mathbf{W}][\\mathbf{M}] = [\\mathbf{T}]$ with\n$[\\mathbf{Q}]^{\\mathrm{T}} \\in \\mathbb{R}^{p \\times p}$ and\n$[\\mathbf{T}]\\in \\mathbb{R}^{p \\times \\sum_{i=1}^{d} i}$, which has upper\ntrapezoidal form. Applying $[\\mathbf{Q}]^{\\mathrm{T}}$ to the system of equations leads:\n\n\\begin{align}[\\mathbf{Q}]^{\\mathrm{T}}[\\mathbf{W}][\\mathbf{D}] \\mathbf{g} =\n    [\\mathbf{Q}]^{\\mathrm{T}}[\\mathbf{W}]\\delta\\mathbf{f} - [\\mathbf{T}]\\mathbf{z}\\end{align}\n\n\nBecause $[\\mathbf{T}]$ is of upper trapezoidal form, one can eliminate the influence\nof the second order derivatives in $\\mathbf{z}$ by discarding the first $\\sum_{i=1}^{d} i$\nequations. The least square solution of the remaining $p-\\sum_{i=1}^{d} i$ equations then provides\na second order accurate estimate of the gradient $\\mathbf{g}$.\n\n\\begin{align}\\mathbf{g} = \\left( [\\mathbf{Q}]^{\\mathrm{T}}[\\mathbf{W}][\\mathbf{D}]\n    \\right)^+[\\mathbf{Q}]^{\\mathrm{T}}[\\mathbf{W}]\\delta\\mathbf{f}\\end{align}\n\nThis approach is more accurate than the first order approximation but needs more sampling points\nbecause of reduction of the system.\n\n![](../../../examples/images/FD_2nd.png)\n\n    :width: 250\n    :align: center\n\nAlthough the initial thought might be that the ordering of the equations would have some impact\non the gradient estimation process, this is indeed not the case. To see why, let\n$[\\mathbf{R}] \\in \\mathbb{R}^{p \\times p}$ be a perturbation matrix that\npermutes the rows of $[\\mathbf{W}][\\mathbf{M}]$. Because the orthogonal reduction\nof $[\\mathbf{W}][\\mathbf{M}]$ produces unique matrices $[\\mathbf{Q}]$\nand $[\\mathbf{T}]$ such that $[\\mathbf{Q}]^{\\mathrm{T}}[\\mathbf{W}][\\mathbf{M}] = [\\mathbf{T}]$\nit follows that applying orthogonal reduction to the permuted system\n$[\\mathbf{R}][\\mathbf{W}][\\mathbf{M}]\\mathbf{x} = \\delta \\mathbf{f}$\nyields with $[\\tilde{\\mathbf{Q}}]^{\\mathrm{T}}[\\mathbf{R}][\\mathbf{W}][\\mathbf{M}] = [\\mathbf{T}]$\nand $[\\mathbf{Q}] = [\\mathbf{R}]^\\mathrm{T}[\\tilde{\\mathbf{Q}}]$ exactly the same system as before.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Comparison between the gradient estimation techniques\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pygpc\nfrom collections import OrderedDict\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Circle\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are going to compare the forward approximation method (most exact but needs additional simulations) with the\nfirst and second order approximations. For each method, we define different distances/radii $dx$:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "methods = [\"FD_fwd\", \"FD_1st\", \"FD_2nd\"]\ndx = [1e-3, 0.1, 0.2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are going to compare the methods using the \"Peaks\" function and we are defining\nthe parameter space by setting up the problem:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# define model\nmodel = pygpc.testfunctions.Peaks()\n\n# define problem\nparameters = OrderedDict()\nparameters[\"x1\"] = pygpc.Beta(pdf_shape=[1, 1], pdf_limits=[1.2, 2])\nparameters[\"x2\"] = 1.\nparameters[\"x3\"] = pygpc.Beta(pdf_shape=[1, 1], pdf_limits=[0, 0.6])\nproblem = pygpc.Problem(model, parameters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Depending on the grid and its density, the methods will behave differently.\nHere, we use 100 random sampling points in the parameter space defined before.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# define grid\nn_grid = 100\ngrid = pygpc.Random(parameters_random=problem.parameters_random,\n                    n_grid=n_grid,\n                    seed=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are setting up a Computation instance to evaluate the model function in the 100 grid points\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# initializing Computation class\ncom = pygpc.Computation(n_cpu=0, matlab_model=False)\n\n# evaluating model function\nres = com.run(model=model,\n              problem=problem,\n              coords=grid.coords,\n              coords_norm=grid.coords_norm,\n              i_iter=None,\n              i_subiter=None,\n              fn_results=None,\n              print_func_time=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are looping over the different methods and evaluate the gradients. The forward approximation method \"FD_fwd\"\nreturns the gradient for every grid point whereas the first and second order approximation \"FD_1st\" and \"FD_2nd\"\nonly return the gradient in grid points if they have sufficient number of neighboring points within radius\n$dx$. The indices stored in \"gradient_idx\" are the indices of the grid points where the gradients are computed.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(columns=[\"method\", \"nrmsd\", \"coverage\"])\ngrad_res = dict()\ngradient_idx = dict()\n\n# determine gradient with different methods\nfor i_m, m in enumerate(methods):\n    # [n_grid x n_out x dim]\n    grad_res[m], gradient_idx[m] = pygpc.get_gradient(model=model,\n                                                      problem=problem,\n                                                      grid=grid,\n                                                      results=res,\n                                                      com=com,\n                                                      method=m,\n                                                      gradient_results_present=None,\n                                                      gradient_idx_skip=None,\n                                                      i_iter=None,\n                                                      i_subiter=None,\n                                                      print_func_time=False,\n                                                      dx=dx[i_m],\n                                                      distance_weight=-2)\n\n    if m != \"FD_fwd\":\n        df.loc[i_m, \"method\"] = m\n        if grad_res[m] is not None:\n            df.loc[i_m, \"coverage\"] = grad_res[m].shape[0]/n_grid\n            df.loc[i_m, \"nrmsd\"] = pygpc.nrmsd(grad_res[m][:, 0, :], grad_res[\"FD_fwd\"][gradient_idx[m], 0, :])\n        else:\n            df.loc[i_m, \"coverage\"] = 0\n            df.loc[i_m, \"nrmsd\"] = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plotting the results\n^^^^^^^^^^^^^^^^^^^^\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# plot results\nfig1, ax1 = plt.subplots(nrows=1, ncols=1, squeeze=True, figsize=(7.5, 5))\n\nn_x = 250\nx1, x2 = np.meshgrid(np.linspace(-1, 1, n_x), np.linspace(-1, 1, n_x))\nx1x2_norm = np.hstack((x1.flatten()[:, np.newaxis], x2.flatten()[:, np.newaxis]))\nx1x2 = grid.get_denormalized_coordinates(x1x2_norm)\n\nres = com.run(model=model,\n              problem=problem,\n              coords=x1x2,\n              coords_norm=x1x2_norm,\n              i_iter=None,\n              i_subiter=None,\n              fn_results=None,\n              print_func_time=False)\n\nim = ax1.pcolor(x1, x2, np.reshape(res, (n_x, n_x), order='c'), cmap=\"jet\")\n\nax1.scatter(grid.coords_norm[:, 0], grid.coords_norm[:, 1], s=1, c=\"k\")\n\nfor i_m, m in enumerate(methods):\n    if m != \"FD_fwd\" and gradient_idx[m] is not None:\n        ax1.scatter(grid.coords_norm[gradient_idx[m], 0],\n                    grid.coords_norm[gradient_idx[m], 1],\n                    s=40, edgecolors=\"w\",\n                    color=sns.color_palette(\"muted\", len(methods)-1)[i_m-1])\n\nax1.legend([\"model function\"] + methods, loc='upper left', bbox_to_anchor=(1, 1)) #,\n\nfor i_m, m in enumerate(methods):\n    if m != \"FD_fwd\" and gradient_idx[m] is not None:\n\n        for i in gradient_idx[m]:\n            circ = Circle((grid.coords_norm[i, 0],\n                           grid.coords_norm[i, 1]),\n                          dx[i_m],\n                          linestyle=\"--\",\n                          linewidth=1.2,\n                          color=\"w\", fill=True, alpha=.1)\n            ax1.add_patch(circ)\n            circ = Circle((grid.coords_norm[i, 0],\n                           grid.coords_norm[i, 1]),\n                          dx[i_m],\n                          linestyle=\"--\",\n                          linewidth=1.2,\n                          edgecolor=sns.color_palette(\"muted\", len(methods)-1)[i_m-1], fill=False,alpha=1)\n            ax1.add_patch(circ)\n\nax1.set_xlabel('$x_1$', fontsize=16)\nax1.set_ylabel('$x_2$', fontsize=16)\nax1.set_xlim([-1, 1])\nax1.set_ylim([-1, 1])\nax1.set_aspect(1.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Comparing the normalized root mean square deviation of the first and second order approximation\nmethods with respect to the forward approximation it can be seen that the 2nd order approximation is more exact.\nHowever, less points could be estimated because of the necessity to eliminate the first 3 equations.\nThis is reflected in the lower coverage\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# show summary\nprint(df)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}