{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nGrid: Random vs LHS\n===================\n\nChoosing a sampling scheme\n--------------------------\n\nTo calculate the coefficients of the gPC matrix, a number of random samples needs to be\npicked to represent the propability space $\\Theta$ and enable descrete evaluations of the\npolynomials. As for the computation of the coefficients, the input parameters $\\mathbf{\\xi}$\ncan be sampled in a number of different ways. In **pygpc** the grid $\\mathcal{G}$ for this\napplication is constructed in `pygpc/Grid.py <../../../../pygpc/Grid.py>`_.\n\nRandom Sampling\n^^^^^^^^^^^^^^^\nIn the case of random sampling the samples will be randomly from their Probability Density Function (PDF)\n$f(\\xi)$.\n\nLatin Hypercube Sampling (LHS)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTo increase the information of each individual sampling point and to prevent undersampling, LHS is a simple\nalternative to enhance the space-filling properties of the sampling scheme first established by\nMcKay et al. (2000).\n\n.. [1] McKay, M. D., Beckman, R. J., & Conover, W. J. (2000). A comparison of three methods for selecting\n   values of input variables in the analysis of output from a computer code. Technometrics, 42(1), 55-61.\n\nTo draw $n$ independent samples from a number of $d$-dimensional parameters\na matrix $\\Pi$ is constructed with\n\n\\begin{align}\\pi_{ij} = \\frac{p_{ij} - u}{n}\\end{align}\n\nwhere $P$ is a $d \\times n$ matrix of randomly perturbed integers\n$p_{ij} \\in \\mathbb{N}, {1,...,n}$ and u is uniform random number $u \\in [0,1]$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Constructing a simple LHS design\n--------------------------------\nWe are going to create a simple LHS design for 2 random variables with 5 sampling points:\nsphinx_gallery_thumbnail_number = 2:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pygpc\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom collections import OrderedDict\n\n# define parameters\nparameters = OrderedDict()\nparameters[\"x1\"] = pygpc.Beta(pdf_shape=[1, 1], pdf_limits=[-np.pi, np.pi])\nparameters[\"x2\"] = pygpc.Beta(pdf_shape=[1, 1], pdf_limits=[-np.pi, np.pi])\n\n# define grid\nlhs = pygpc.LHS(parameters_random=parameters, n_grid=0)\n\n# draw samples\npi = lhs.get_lhs_grid(dim=2, n=25)\n\n# plot\nfig = plt.figure(figsize=(4, 4))\nplt.scatter(pi[:,0], pi[:,1])\nplt.xlabel(\"$x_1$\", fontsize=12)\nplt.ylabel(\"$x_2$\", fontsize=12)\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.grid()\nplt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "LHS Designs can further be improved upon, since the pseudo-random sampling procedure\ncan lead to samples with high spurious correlation and the space filling capability\nin itself leaves room for improvement, some optimization criteria have been found to\nbe adequate for compensating the initial designs shortcomings.\n\nOptimization Criteria of LHS designs\n------------------------------------\nSpearman Rank Correlation\n^^^^^^^^^^^^^^^^^^^^^^^^^\nFor a sample size of $n$ the scores of each variable are converted to their Ranks $rg_{X_i}$\nthe Spearman Rank Correlation Coefficient is then the Pearson Correlation Coefficient applied to the rank\nvariables $rg_{X_i}$:\n\n\\begin{align}r_s = \\rho_{rg_{X_i}, rg_{X_j}} = \\frac{cov(rg_{X_i}, rg_{X_j})}{\\sigma_{rg_{X_i}} \\sigma_{rg_{X_i}}}\\end{align}\n\nwhere $\\rho$ is the pearson correlation coefficient, $\\sigma$ is the standard deviation\nand $cov$ is the covariance of the rank variables\n\nMaximum-Minimal-Distance\n^^^^^^^^^^^^^^^^^^^^^^^^\nFor creating a so called maximin distance design that maximizes the minimum inter-site distance, proposed by\nJohnson et al.\n\n\\begin{align}\\min_{1 \\leqslant i, j \\leqslant n, i \\neq j} d(x_i,x_j),\\end{align}\n\nwhere $d$ is the distance between two samples $x_i$ and $x_j$ and\n$n$ is the number of samples in a sample design.\n\n\\begin{align}d(x_i,x_j) = d_ij = [ \\sum_{k=1}^{m}|x_ik - x_jk| ^ t]^\\frac{1}{t}, t \\in {1,2}\\end{align}\n\nThere is however a more elegant way of computing this optimization criterion as shown by Morris and Mitchell (1995),\ncalled the $\\varphi_P$ criterion.\n\n\\begin{align}\\min\\varphi_P \\quad \\text{subject to} \\quad \\varphi_P = [ \\sum_{k = 1} ^ {s} J_id_i  ^ p]^\\frac{1}{p},\\end{align}\n\nwhere $s$ is the number of distinct distances, $J$ is an vector of indices of the distances\nand $p$ is an integer. With a very large $p$ this criterion is equivalent to the maximin criterion\n\n.. Morris, M. D. and Mitchell, T. J. ( (1995). Exploratory Designs for Computer Experiments.J. Statist. Plann.\n   Inference 43, 381-402.\n\nLHS with enhanced stochastic evolutionary algorithm (ESE)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTo achieve optimized designs with a more stable method and possibly quicker then by simply evaluating\nthe criteria over a number of repetitions **pygpc** can use an ESE for achieving sufficient\n$\\varphi_P$-value. This algorithm is more appealing in its efficacy and proves to\n[sth about the resulting error or std in a low sample size].\nThis method originated from Jin et al. (2005).\n\n.. Jin, R., Chen, W., Sudjianto, A. (2005). An efficient algorithm for constructing optimal\n   design of computer experiments. Journal of statistical planning and inference, 134(1), 268-287.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Comparison between a standard random grid and different LHS designs\n-------------------------------------------------------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from scipy.stats import spearmanr\nimport seaborn as sns\n\n# define parameters\nparameters = OrderedDict()\nparameters[\"x1\"] = pygpc.Beta(pdf_shape=[1, 1], pdf_limits=[-np.pi, np.pi])\nparameters[\"x2\"] = pygpc.Beta(pdf_shape=[1, 1], pdf_limits=[-np.pi, np.pi])\n\n# define grids for each criteria\nlhs_basic = pygpc.LHS(parameters_random=parameters, n_grid=0)\nlhs_corr = pygpc.LHS(parameters_random=parameters, n_grid=0)\nlhs_maximin = pygpc.LHS(parameters_random=parameters, n_grid=0, options='maximin')\nlhs_ese = pygpc.LHS(parameters_random=parameters, n_grid=0, options='ese')\n\n# draw samples\ndim = 5\nn = 30\nsamples = []\n\nsamples.append(np.random.rand(n, dim))\nsamples.append(lhs_basic.get_lhs_grid(dim, n))\nsamples.append(lhs_corr.get_lhs_grid(dim, n, crit='corr'))\nsamples.append(lhs_maximin.get_lhs_grid(dim, n, crit='maximin'))\nsamples.append(lhs_ese.get_lhs_grid(dim, n, crit='ese'))\n\n# calculate criteria\ncorrs = []\nphis = []\nname = []\nvariables = []\n\nfor i in range(5):\n    corr = spearmanr(samples[i][:, 0], samples[i][:, 1])[0]\n    corrs.append(corr)\n\nfor i in range(5):\n    phip = lhs_basic.PhiP(samples[i])\n    phis.append(phip)\n\nvariables.append(corrs)\nname.append('corr')\nvariables.append(phis)\nname.append('phi')\n\n# plot results\nfig = plt.figure(figsize=(16, 3))\ntitles = ['Random', 'LHS (standard)', 'LHS (corr opt)', 'LHS (Phi-P opt)', 'LHS (ESE)']\n\nfor i in range(5):\n    text = name[0] + ' = {:0.2f} '.format(variables[0][i]) + \"\\n\" + \\\n           name[1] + ' = {:0.2f}'.format(variables[1][i])\n    plot_index = 151 + i\n    plt.gcf().text((0.15 + i * 0.16), 0.08, text, fontsize=14)\n    plt.subplot(plot_index)\n    plt.scatter(samples[i][:, 0], samples[i][:, 1], color=sns.color_palette(\"bright\", 5)[i])\n    plt.title(titles[i])\n    plt.gca().set_aspect('equal', adjustable='box')\nplt.subplots_adjust(bottom=0.3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The initial LHS (standard) has already good space filling properties compared\nto the random sampling scheme (eg. less under sampled areas and less clustered areas,\nvisually and quantitatively represented by the optimization criteria). The LHS (ESE)\nshows the best correlation and $\\varphi_P$ criterion.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Convergence and stability comparison in gPC\n-------------------------------------------\nWe are going to compare the different grids in a practical gPC example considering the Ishigami function.\nWe are going to conduct gPC analysis for different approximation orders (grid sizes).\nBecause we are working with random grids, we are interested in (i) the rate of convergence\nand (ii) the stability of the convergence. For that reason, we will repeat the analysis several times.\n\nSetting up the problem\n^^^^^^^^^^^^^^^^^^^^^^\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pygpc\nimport numpy as np\nfrom collections import OrderedDict\nimport matplotlib.pyplot as plt\n\n# grids to compare\ngrids = [pygpc.Random, pygpc.LHS, pygpc.LHS, pygpc.LHS, pygpc.LHS]\ngrids_options = [None, None, \"corr\", \"maximin\", \"ese\"]\ngrid_legend = [\"Random\", \"LHS (standard)\", \"LHS (corr opt)\", \"LHS (Phi-P opt)\", \"LHS (ESE)\"]\norder = [2, 3, 4, 5, 6, 7, 8, 9, 10]\nrepetitions = 5\n\nerr = np.zeros((len(grids), len(order), repetitions))\nn_grid = np.zeros(len(order))\n\n# Model\nmodel = pygpc.testfunctions.Ishigami()\n\n# Problem\nparameters = OrderedDict()\nparameters[\"x1\"] = pygpc.Beta(pdf_shape=[1, 1], pdf_limits=[-np.pi, np.pi])\nparameters[\"x2\"] = pygpc.Beta(pdf_shape=[1, 1], pdf_limits=[-np.pi, np.pi])\nparameters[\"x3\"] = 0.\nparameters[\"a\"] = 7.\nparameters[\"b\"] = 0.1\n\nproblem = pygpc.Problem(model, parameters)\n\n# gPC options\noptions = dict()\noptions[\"method\"] = \"reg\"\noptions[\"solver\"] = \"Moore-Penrose\"\noptions[\"interaction_order\"] = problem.dim\noptions[\"order_max_norm\"] = 1\noptions[\"n_cpu\"] = 0\noptions[\"adaptive_sampling\"] = False\noptions[\"gradient_enhanced\"] = False\noptions[\"fn_results\"] = None\noptions[\"error_type\"] = \"nrmsd\"\noptions[\"error_norm\"] = \"relative\"\noptions[\"matrix_ratio\"] = 2\noptions[\"eps\"] = 0.001\noptions[\"backend\"] = \"omp\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Running the analysis\n^^^^^^^^^^^^^^^^^^^^\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for i_g, g in enumerate(grids):\n    for i_o, o in enumerate(order):\n        for i_n, n in enumerate(range(repetitions)):\n\n            options[\"order\"] = [o] * problem.dim\n            options[\"order_max\"] = o\n            options[\"grid\"] = g\n            options[\"grid_options\"] = grids_options[i_g]\n\n            n_coeffs = pygpc.get_num_coeffs_sparse(order_dim_max=options[\"order\"],\n                                                   order_glob_max=options[\"order_max\"],\n                                                   order_inter_max=options[\"interaction_order\"],\n                                                   dim=problem.dim)\n\n            grid = g(parameters_random=problem.parameters_random,\n                     n_grid=options[\"matrix_ratio\"] * n_coeffs,\n                     options=options[\"grid_options\"])\n\n            # define algorithm\n            algorithm = pygpc.Static(problem=problem, options=options, grid=grid)\n\n            # Initialize gPC Session\n            session = pygpc.Session(algorithm=algorithm)\n\n            # run gPC session\n            session, coeffs, results = session.run()\n\n            err[i_g, i_o, i_n] = pygpc.validate_gpc_mc(session=session,\n                                                       coeffs=coeffs,\n                                                       n_samples=int(1e4),\n                                                       n_cpu=0,\n                                                       output_idx=0,\n                                                       fn_out=None,\n                                                       plot=False)\n\n        n_grid[i_o] = grid.n_grid\n\nerr_mean = np.mean(err, axis=2)\nerr_std = np.std(err, axis=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Results\n^^^^^^^\nEven after a small set of repetitions the $\\varphi_P$ optimizing ESE will produce\nthe best results regarding the aforementioned criteria, while also having less variation\nin its pseudo-random design. Thus is it possible to half the the root-mean-squared error\n$\\varepsilon$ by using the ESE algorithm compared to completely random sampling the\ngrid points, while also having a consistently small standard deviation.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=[12,5])\n\nfor i in range(len(grids)):\n    ax[0].errorbar(n_grid, err_mean[i, :], err_std[i, :], capsize=3, elinewidth=.5)\n    ax[1].plot(n_grid, err_std[i, :])\n\nfor a in ax:\n    a.legend(grid_legend)\n    a.set_xlabel(\"$N_g$\", fontsize=12)\n    a.grid()\n\nax[0].set_ylabel(\"$\\epsilon$\", fontsize=12)\nax[1].set_ylabel(\"std($\\epsilon$)\", fontsize=12)\n\nax[0].set_title(\"gPC error vs original model (mean and std)\")\n_ = ax[1].set_title(\"gPC error vs original model (std)\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}