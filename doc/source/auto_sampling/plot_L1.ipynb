{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# L1 optimal sampling\n\nBefore explaining the different types of L1 optimal grids a brief motivation for the L1-optimization and\nfurther the L1 optimal sampling that aims to strengthen the benefit from this procedure is given. L1 optimization is\nused for solving the following linear algebra problem at the core of the gPC for underdetermined system where\nthe number of model evaluations is less than the number of gPC coefficients to be determined.\n\n\\begin{align}\\mathbf{Y_{M}} = \\mathbf{\\Psi_{M \\times N}} \\mathbf{c_{N}}\\end{align}\n\nIn this case the matrix $\\mathbf{\\Psi}$ is of size $M\\times N$ and the coefficient vector $\\mathbf{C}$ of\nsize $N$, where $N<M$. In other words we're trying to fit more coefficients then we have data points. This\nprocedure is most effective if the vector or array of coefficients has a high amount of vanishing and thus not needed\nentries. This type of problem can also be called sparse recovery or compressive sensing.\n\nL1 optimal sampling seeks to tune the grid composition for solving such a problem efficiently.\nMost grids in this category are based on coherence optimal samples drawn from a sampling strategy introduced by Hampton\nand Doostan (2015) in the framework of gPC.\n\nA variety of grid types can be build upon this idea:\n\n## Coherence optimal sampling (CO)\n\nCoherence optimal sampling seeks to minimize the spectral matrix norm between the Gram matrix of the gPC matrix\nand the identify matrix. The Gram matrix is defined by:\n\n\\begin{align}\\mathbf{G_\\Psi} = \\frac{1}{N_g}[\\mathbf{\\Psi^T}] [\\mathbf{\\Psi}]\\end{align}\n\nand its distance from the identity matrix by:\n\n\\begin{align}\\end{align}\n ||\\mathbf{G_\\Psi}-\\mathbf{I}||\n\nThis objective is usually sidestepped by minimizing the coherence parameter $\\mu$ instead:\n\n\\begin{align}\\mu = \\sup_{\\mathbf{\\xi}\\in\\Omega} \\sum_{j=1}^P |w(\\mathbf{\\xi})\\psi_j(\\mathbf{\\xi})|^2\\end{align}\n\nwhere $\\xi$ are the random variables representing the input parameters, $\\Omega$ is the entirety of all\nrandom variables $w$ are weighting functions discussed below and $\\psi_j$ are the elements of the matrix\n$\\mathbf{\\Psi}$. The minimization can be realized by sampling the input parameters with an alternative\ndistribution:\n\n\\begin{align}P_{\\mathbf{Y}}(\\mathbf{\\xi}) := c^2 P(\\mathbf{\\xi}) B^2(\\mathbf{\\xi})\\end{align}\n\nwhere $c$ is a normalization constant, $P(\\mathbf{\\xi})$ is the joint probability density function of the\noriginal input distributions and $B(\\mathbf{\\xi})$ is an upper bound of the polynomial chaos basis:\n\n\\begin{align}B(\\mathbf{\\xi}):= \\sqrt{\\sum_{j=1}^P|\\psi_j(\\mathbf{\\xi})|^2}\\end{align}\n\nTo avoid defining the normalization constant $c$ a Markov Chain Monte Carlo approach using a Metropolis-Hastings sampler\n[2] is used to draw samples from $P_{\\mathbf{Y}}(\\mathbf{\\xi})$. For the Mertopolis-Hastings sampler it is necessary\nto define a sufficient candidate distribution. For a coherence optimal sampling this is realized by a proposal\ndistribution $g(\\xi)$ (see the method introduced by Hampton). By sampling from a different distribution then\n:math:'P(\\xi)' however it is not possible to guarantee $\\mathbf{\\Psi}$ to be a matrix of orthonormal\npolynomials.\n\nTherefore $\\mathbf{W}$ needs to be a diagonal positive-definite matrix of weight-functions $w(\\xi)$ which\nis then applied to:\n\n\\begin{align}\\mathbf{W} \\mathbf{Y} =  \\mathbf{W} \\mathbf{\\Psi}\\mathbf{c}\\end{align}\n\nIn practice it is possible to compute $\\mathbf{W}$ with:\n\n\\begin{align}w_i(\\xi) = \\frac{1}{B_i(\\xi)}\\end{align}\n\n### Example\nIn order to create a coherence optimal grid of sampling points, we have to define the random parameters and create\na gpc object.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pygpc\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom collections import OrderedDict\n\n# define model\nmodel = pygpc.testfunctions.RosenbrockFunction()\n\n# define parameters\nparameters = OrderedDict()\nparameters[\"x1\"] = pygpc.Beta(pdf_shape=[1, 1], pdf_limits=[-np.pi, np.pi])\nparameters[\"x2\"] = pygpc.Beta(pdf_shape=[1, 1], pdf_limits=[-np.pi, np.pi])\n\n# define problem\nproblem = pygpc.Problem(model, parameters)\n\n# create gpc object\ngpc = pygpc.Reg(problem=problem,\n                order=[5]*problem.dim,\n                order_max=5,\n                order_max_norm=1,\n                interaction_order=2,\n                interaction_order_current=2,\n                options=None,\n                validation=None)\n\n# create a coherence optimal grid\ngrid_co = pygpc.CO(parameters_random=parameters,\n                   n_grid=50,\n                   gpc=gpc,\n                   options={\"seed\": None,\n                            \"n_warmup\": 1000})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "An example of how the samples are distributed in the probability space is given below:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.scatter(grid_co.coords_norm[:, 0], grid_co.coords_norm[:, 1],\n              color=sns.color_palette(\"bright\", 5)[0])\n\nplt.xlabel(\"$x_1$\", fontsize=12)\nplt.ylabel(\"$x_2$\", fontsize=12)\nplt.xticks(np.linspace(-1, 1, 5))\nplt.yticks(np.linspace(-1, 1, 5))\nplt.xlim([-1, 1])\nplt.ylim([-1, 1])\nplt.title(\"CO\")\nplt.grid()\nplt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Mutual coherence optimal sampling (mc)\n\nThe mutual coherence of a matrix measures the cross-correlations between its columns by evaluating the largest\nabsolute and normalized inner product between different columns. It is given by:\n\n\\begin{align}\\mu(\\mathbf{\\Psi}) = \\max_ {1 \\leq i, j\\leq N_c, j\\neq i} \\quad \\frac{|\\psi_i^T \\psi_j|}{||\\psi_i||_2||\n    \\psi_j||_2}\\end{align}\n\nThe objective is to select sampling points to minimize $\\mu(\\mathbf{\\Psi})$ for a desired L1 optimal design.\nMinimizing the mutual-coherence considers only the worst-case scenario and does not account to improve\ncompressive sampling performance in general.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# create a mutual coherence optimal grid\ngrid_mc = pygpc.L1(parameters_random=parameters,\n                   n_grid=50,\n                   gpc=gpc,\n                   options={\"criterion\": [\"mc\"],\n                            \"method\": \"greedy\",\n                            \"n_pool\": 1000,\n                            \"seed\": None})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "An example of how the samples are distributed in the probability space is given below:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.scatter(grid_mc.coords_norm[:, 0], grid_mc.coords_norm[:, 1],\n              color=sns.color_palette(\"bright\", 5)[0])\n\nplt.xlabel(\"$x_1$\", fontsize=12)\nplt.ylabel(\"$x_2$\", fontsize=12)\nplt.xticks(np.linspace(-1, 1, 5))\nplt.yticks(np.linspace(-1, 1, 5))\nplt.xlim([-1, 1])\nplt.ylim([-1, 1])\nplt.title(\"MC\")\nplt.grid()\nplt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Mutual coherence and average cross-correlation optimal sampling (mc-cc)\n\nAn improvement of sampling designs that are only optimized in the mutual coherence is done by adding the average cross-\ncorrelation as a measure for a two-fold optimization with the benefit of further robustness in its efficient sparse\nrecovery. The average cross-correlation is defined by:\n\n\\begin{align}\\gamma(\\mathbf{\\Psi}) = \\frac{1}{N} \\min_{\\mathbf{\\Psi} \\in R^{M \\times N_c}} ||I_{N_c} -\n    \\mathbf{G_\\mathbf{\\Psi}}||^2_F\\end{align}\n\nwhere $||\\cdot||_F$ denotes the Frobenius norm and $N := K \\times (K - 1)$ is the total number of column\npairs. In this context, Alemazkoor and Meidani (2018) proposed a hybrid optimization criteria, which minimizes the\naverage-cross correlation $\\gamma(\\mathbf{\\Psi})$ and the mutual coherence $\\mu(\\mathbf{\\Psi})$:\n\n\\begin{align}\\text{argmin}\\left(f\\left(\\mathbf{\\Psi}\\right)\\right) = \\text{argmin}\\left(\\left(\\frac{\\mu_{i} -\\min\n    \\left(\\boldsymbol\\mu \\right)}{\\max \\left (\\boldsymbol\\mu \\right)\n     - \\min \\left(\\boldsymbol\\mu \\right)} \\right)^2 + \\left(\\frac{\\gamma_i -\\min \\left(\\boldsymbol\\gamma \\right)}{\\max \\left(\\boldsymbol\\gamma \\right)\n     - \\min \\left(\\boldsymbol\\gamma \\right)} \\right)^2 \\right)\\end{align}\n\nwith $\\boldsymbol\\mu = (\\mu_{1}, \\mu_{2}, ..., \\mu_{i})$ and $\\boldsymbol\\gamma = (\\gamma_1,\n\\gamma_2, ..., \\gamma_i)$\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# create a mutual coherence and cross correlation optimal grid\ngrid_mc_cc = pygpc.L1(parameters_random=parameters,\n                      n_grid=50,\n                      gpc=gpc,\n                      options={\"criterion\": [\"mc\", \"cc\"],\n                               \"method\": \"greedy\",\n                               \"n_pool\": 1000,\n                               \"seed\": None})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "An example of how the samples are distributed in the probability space is given below:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.scatter(grid_mc_cc.coords_norm[:, 0], grid_mc_cc.coords_norm[:, 1],\n              color=sns.color_palette(\"bright\", 5)[0])\n\nplt.xlabel(\"$x_1$\", fontsize=12)\nplt.ylabel(\"$x_2$\", fontsize=12)\nplt.xticks(np.linspace(-1, 1, 5))\nplt.yticks(np.linspace(-1, 1, 5))\nplt.xlim([-1, 1])\nplt.ylim([-1, 1])\nplt.title(\"MC-CC\")\nplt.grid()\nplt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## D-optimal sampling\n\nFurther a selection of optimization criteria derived from $\\mathbf{G_\\Psi}$ and the identification of\ncorresponding optimal sampling locations is the core concept of optimal design of experiment (ODE). The most popular\ncriterion for that is $D$-optimality where it the goal to increase the information content from a given amount of\nsampling points by minimizing the determinant of the inverse of the Gramian:\n\n\\begin{align}\\phi_D = |\\mathbf{G_\\Psi}^{-1}|^{1/N_c}\\end{align}\n\n$D$-optimal designs are focused on precise estimation of the coefficients. Besides $D$-optimal designs,\nthere exist are a lot of other alphabetic optimal designs such as $A$-, $E$-, $I$-, or $V$-\noptimal designs with different goals and criteria. A nice overview about them can be found by Atkinson (2007) and\nPukelsheim (2006).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# create a D optimal grid\ngrid_d = pygpc.L1(parameters_random=parameters,\n                      n_grid=50,\n                      gpc=gpc,\n                      options={\"criterion\": [\"D\"],\n                               \"method\": \"greedy\",\n                               \"n_pool\": 1000,\n                               \"seed\": None})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "An example of how the samples are distributed in the probability space is given below:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.scatter(grid_d.coords_norm[:, 0], grid_d.coords_norm[:, 1],\n              color=sns.color_palette(\"bright\", 5)[0])\n\nplt.xlabel(\"$x_1$\", fontsize=12)\nplt.ylabel(\"$x_2$\", fontsize=12)\nplt.xticks(np.linspace(-1, 1, 5))\nplt.yticks(np.linspace(-1, 1, 5))\nplt.xlim([-1, 1])\nplt.ylim([-1, 1])\nplt.title(\"D\")\nplt.grid()\nplt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## D-coherence optimal sampling\n\n$D$-optimal designs can even be combined with coherence optimal designs by using a pool of already coherence\noptimal samples and then applying the optimization of the $D$ criterion on it. This has been shown to be a\npromising approach for special cases of functions by Diaz et al. (2017). For that and the other L1 optimal sampling schemes we\nused a greedy algorithm to determine the sets of sampling points. In this algorithm, we generate a pool of coherence\noptimal samples using the Metropolis-Hastings sampler and randomly pick an initial sample. In the next iteration we\nsuccessively add a sampling point and calculate the respective optimization criteria. After evaluating all possible\ncandidates, we add the sampling point yielding the best criterion and append it to the existing set. This is repeated\nuntil the sampling set has the desired size.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# create a D-coherence optimal grid\ngrid_d_coh = pygpc.L1(parameters_random=parameters,\n                      n_grid=50,\n                      gpc=gpc,\n                      options={\"criterion\": [\"D-coh\"],\n                               \"method\": \"greedy\",\n                               \"n_pool\": 1000,\n                               \"seed\": None})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "An example of how the samples are distributed in the probability space is given below:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.scatter(grid_d_coh.coords_norm[:, 0], grid_d_coh.coords_norm[:, 1],\n              color=sns.color_palette(\"bright\", 5)[0])\n\nplt.xlabel(\"$x_1$\", fontsize=12)\nplt.ylabel(\"$x_2$\", fontsize=12)\nplt.xticks(np.linspace(-1, 1, 5))\nplt.yticks(np.linspace(-1, 1, 5))\nplt.xlim([-1, 1])\nplt.ylim([-1, 1])\nplt.title(\"D-Coh\")\nplt.grid()\nplt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "L1 designs with different optimization criteria can be created using the \"criterion\" argument in the options\ndictionary.\n\n## Options\n\nThe following options are available for L1-optimal grids:\n\n**pygpc.CO()**\n\n- seed: set a seed to reproduce the results (default: None)\n- n_warmup: the number of samples that are discarded in the Metropolis-Hastings sampler before samples are accepted (default: max(200, 2*n_grid), here n_grid is the amount of samples that are meant to be generated)\n\n**pygpc.L1()**\n\n- seed: set a seed to reproduce the results (default: None)\n- method:\n   - \"greedy\": greedy algorithm (default, recommended)\n   - \"iter\": iterative algorithm (faster but does not perform as good as \"greedy\")\n- criterion:\n   - [\"mc\"]: mutual coherence optimal\n   - [\"mc\", \"cc\"]: mutual coherence and cross correlation optimal\n   - [\"D\"]: D optimal\n   - [\"D-coh\"]: D and coherence optimal\n- n_pool: number of grid points in overall pool to select optimal points from (default: 10.000)\n- n_iter: number of iterations used for the \"iter\" method (default: 1000)\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The sampling method can be selected accordingly for each gPC algorithm by setting the following options\nwhen setting up the algorithm:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "options = dict()\n...\noptions[\"grid\"] = pygpc.L1\noptions[\"grid_options\"] = {\"seed\": None,\n                           \"method\": \"greedy\",\n                           \"criterion\": [\"mc\", \"cc\"],\n                           \"n_pool\": 1000}\n..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References\n.. [1] Hampton, J., Doostan A., Coherence motivated sampling and convergence analysis of least\n    squares polynomial Chaos regression, Computer Methods in Applied Mechanics and Engineering,\n    290 (2015), 73\u201397.\n.. [2] Hastings, W. K., Monte Carlo sampling methods using Markov chains and their applications,\n    1970.\n.. [3] Alemazkoor N., Meidani, H., A near-optimal sampling strategy for sparse recovery of polynomial\n    chaos expansions, Journal of Computational Physics, 371 (2018), 137\u2013151\n.. [4] Atkinson, A. C., Optimum experimental designs, with SAS, vol. 34 of Oxford statistical\n   science series, Oxford Univ. Press, Oxford, 2007, URL http://site.ebrary.com/lib/academiccompletetitles/home.action.\n.. [5] Pukelsheim, F., Optimal design of experiments, SIAM, 2006.\n.. [6] Diaz, P., Doostan, A., and Hampton, J., Sparse polynomial chaos expansions via compressed sensing\n   and D-optimal design, Computer Methods in Applied Mechanics and Engineering, 336 (2018), 640\u2013666.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# When using Windows you need to encapsulate the code in a main function and insert an\n# if __name__ == '__main__': guard in the main module to avoid creating subprocesses recursively:\n#\n# if __name__ == '__main__':\n#     main()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}