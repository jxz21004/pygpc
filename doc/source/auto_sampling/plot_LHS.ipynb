{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Latin Hypercube Sampling (LHS)\n\nTo increase the information of each individual sampling point and to prevent undersampling, LHS is a simple\nalternative to enhance the space-filling properties of the sampling scheme first established by\nMcKay et al. (2000).\n\n.. [1] McKay, M. D., Beckman, R. J., & Conover, W. J. (2000). A comparison of three methods for selecting\n   values of input variables in the analysis of output from a computer code. Technometrics, 42(1), 55-61.\n\nTo draw $n$ independent samples from a number of $d$-dimensional parameters\na matrix $\\Pi$ is constructed with\n\n\\begin{align}\\pi_{ij} = \\frac{p_{ij} - u}{n}\\end{align}\n\nwhere $P$ is a $d \\times n$ matrix of randomly perturbed integers\n$p_{ij} \\in \\mathbb{N}, {1,...,n}$ and u is uniform random number $u \\in [0,1]$.\n\nLHS Designs can further be improved upon, since the pseudo-random sampling procedure\ncan lead to samples with high spurious correlation and the space filling capability\nin itself leaves room for improvement, some optimization criteria have been found to\nbe adequate for compensating the initial designs shortcomings.\n\n## Optimization Criteria of LHS designs\nSpearman Rank Correlation\n^^^^^^^^^^^^^^^^^^^^^^^^^\nFor a sample size of $n$ the scores of each variable are converted to their Ranks $rg_{X_i}$\nthe Spearman Rank Correlation Coefficient is then the Pearson Correlation Coefficient applied to the rank\nvariables $rg_{X_i}$:\n\n\\begin{align}r_s = \\rho_{rg_{X_i}, rg_{X_j}} = \\frac{cov(rg_{X_i}, rg_{X_j})}{\\sigma_{rg_{X_i}} \\sigma_{rg_{X_i}}}\\end{align}\n\nwhere $\\rho$ is the pearson correlation coefficient, $\\sigma$ is the standard deviation\nand $cov$ is the covariance of the rank variables\n\n### Maximum-Minimal-Distance\nFor creating a so called maximin distance design that maximizes the minimum inter-site distance, proposed by\nJohnson et al.\n\n\\begin{align}\\min_{1 \\leqslant i, j \\leqslant n, i \\neq j} d(x_i,x_j),\\end{align}\n\nwhere $d$ is the distance between two samples $x_i$ and $x_j$ and\n$n$ is the number of samples in a sample design.\n\n\\begin{align}d(x_i,x_j) = d_{i,j} = [ \\sum_{k=1}^{m}|x_{i,k} - x_{j,k}| ^ t]^\\frac{1}{t}, t \\in {1,2}\\end{align}\n\nThere is however a more elegant way of computing this optimization criterion as shown by Morris and Mitchell (1995),\ncalled the $\\varphi_P$ criterion.\n\n\\begin{align}\\min\\varphi_P \\quad \\text{subject to} \\quad \\varphi_P = [ \\sum_{k = 1} ^ {s} J_id_i  ^ p]^\\frac{1}{p},\\end{align}\n\nwhere $s$ is the number of distinct distances, $J$ is an vector of indices of the distances\nand $p$ is an integer. With a very large $p$ this criterion is equivalent to the maximin criterion\n\n.. Morris, M. D. and Mitchell, T. J. (1995). Exploratory Designs for Computer Experiments. J. Statist. Plann.\n   Inference 43, 381-402.\n\n### LHS with enhanced stochastic evolutionary algorithm (ESE)\nTo achieve optimized designs with a more stable method and possibly quicker then by simply evaluating\nthe criteria over a number of repetitions **pygpc** can use an ESE for achieving sufficient\n$\\varphi_P$-value. This algorithm is more appealing in its efficacy and proves to\n[sth about the resulting error or std in a low sample size].\nThis method originated from Jin et al. (2005).\n\n.. Jin, R., Chen, W., Sudjianto, A. (2005). An efficient algorithm for constructing optimal\n   design of computer experiments. Journal of statistical planning and inference, 134(1), 268-287.\n\n## Example\nIn order to create a grid of sampling points, we have to define the random parameters and create a gpc object.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pygpc\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom collections import OrderedDict\n\n# define model\nmodel = pygpc.testfunctions.RosenbrockFunction()\n\n# define parameters\nparameters = OrderedDict()\nparameters[\"x1\"] = pygpc.Beta(pdf_shape=[1, 1], pdf_limits=[-np.pi, np.pi])\nparameters[\"x2\"] = pygpc.Beta(pdf_shape=[1, 1], pdf_limits=[-np.pi, np.pi])\n\n# define problem\nproblem = pygpc.Problem(model, parameters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "LHS designs with different optimization criteria can be created using the \"criterion\" argument in the options\ndictionary. In the following, we are going to create different LHS designs for 2 random variables with 200\nsampling points:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "grid_lhs_std = pygpc.LHS(parameters_random=parameters,\n                         n_grid=200,\n                         options={\"criterion\": None,      \"seed\": None})\ngrid_lhs_cor = pygpc.LHS(parameters_random=parameters,\n                         n_grid=200,\n                         options={\"criterion\": \"corr\",    \"seed\": None})\ngrid_lhs_max = pygpc.LHS(parameters_random=parameters,\n                         n_grid=200,\n                         options={\"criterion\": \"maximin\", \"seed\": None})\ngrid_lhs_ese = pygpc.LHS(parameters_random=parameters,\n                         n_grid=200,\n                         options={\"criterion\": \"ese\",     \"seed\": None})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following options are available for D-optimal grids:\n\n- seed: set a seed to reproduce the results (default: None)\n- criterion:\n   - None: Standard LHS\n   - \"corr\": Correlation optimal LHS\n   - \"maximin\": Maximum-minimum distance optimal LHS\n   - \"ese\": LHS with enhanced stochastic evolutionary algorithm (ESE)\n\nThe grid points are distributed as follows (in the normalized space):\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(nrows=1, ncols=4, squeeze=True, figsize=(12.7, 3.2))\n\nax[0].scatter(grid_lhs_std.coords_norm[:, 0], grid_lhs_std.coords_norm[:, 1], color=sns.color_palette(\"bright\", 5)[0])\nax[1].scatter(grid_lhs_cor.coords_norm[:, 0], grid_lhs_cor.coords_norm[:, 1], color=sns.color_palette(\"bright\", 5)[1])\nax[2].scatter(grid_lhs_max.coords_norm[:, 0], grid_lhs_max.coords_norm[:, 1], color=sns.color_palette(\"bright\", 5)[2])\nax[3].scatter(grid_lhs_ese.coords_norm[:, 0], grid_lhs_ese.coords_norm[:, 1], color=sns.color_palette(\"bright\", 5)[3])\n\ntitle = ['LHS (standard)', 'LHS (corr opt)', 'LHS (Phi-P opt)', 'LHS (ese)']\n\nfor i in range(len(ax)):\n    ax[i].set_xlabel(\"$x_1$\", fontsize=12)\n    ax[i].set_ylabel(\"$x_2$\", fontsize=12)\n    ax[i].set_xticks(np.linspace(-1, 1, 5))\n    ax[i].set_yticks(np.linspace(-1, 1, 5))\n    ax[i].set_xlim([-1, 1])\n    ax[i].set_ylim([-1, 1])\n    ax[i].set_title(title[i])\n    ax[i].grid()\n\nplt.tight_layout()\n\n# When using Windows you need to encapsulate the code in a main function and insert an\n# if __name__ == '__main__': guard in the main module to avoid creating subprocesses recursively:\n#\n# if __name__ == '__main__':\n#     main()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}