{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Dimensionality reduction\n\n## Introduction\nA large number of models show redundancies in the form of correlations between the parameters under investigation.\nEspecially in the case of high-dimensional problems, the correlative behavior of the target variable as a function\nof the parameters is usually completely unknown.  Correlating parameters can be combined into surrogate parameters\nby means of a principal component analysis, which enables a reduction of the effective parameter number.\nThis significantly reduces the computational effort compared to the original problem with only minor losses\nin modeling accuracy.\n\nIn this case, the $n_d$ original random variables $\\mathbf{\\xi}$ are reduced to a new set of $n_d'$\nrandom variables $\\mathbf{\\eta}$, where $n_d'<n_d$ and which are a linear combination of the original\nrandom variables such that:\n\n\\begin{align}\\mathbf{\\eta} = [\\mathbf{W}]\\mathbf{\\xi}\\end{align}\n\nThe challenging part is to determine the projection matrix $[\\mathbf{W}]$, which rotates the basis and\nreformulates the original problem to a more efficient one without affecting the solution accuracy too much.\nDetermining the projection matrix results in a separate optimization problem besides of determining the\ngPC coefficients. Several approaches where investigated in the literature from for example Tipireddy and Ghanem (2014)\nas well as Tsilifis et al. (2019). They solved the coupled optimization problems alternately, i.e. keeping the solution\nof the one fixed while solving the other until some convergence criterion is satisfied.\n\n## Method\nIn pygpc, the projection matrix $[\\mathbf{W}]$ is determined from the singular value decomposition of\nthe gradients of the solution vector along the original parameter space. The gradients of the quantity of interest (QoI)\nalong $\\mathbf{\\xi}$ are stored in the matrix $[\\mathbf{Y}_\\delta]$:\n\n\\begin{align}[\\mathbf{Y}_\\partial] =\n    \\left[ \\begin{array}{ccc}\n    \\left.\\frac{\\partial y}{\\partial\\xi_1}\\right|_{\\xi^{(1)}} & \\ldots & \\left.\\frac{\\partial y}{\\partial\\xi_d}\\right|_{\\xi^{(1)}}\\\\\n    \\left.\\frac{\\partial y}{\\partial\\xi_1}\\right|_{\\xi^{(2)}} & \\ldots & \\left.\\frac{\\partial y}{\\partial\\xi_d}\\right|_{\\xi^{(2)}}\\\\\n    \\vdots & \\ddots & \\vdots\\\\\n    \\left.\\frac{\\partial y}{\\partial\\xi_1}\\right|_{\\xi^{(n_g)}} & \\ldots & \\left.\\frac{\\partial y}{\\partial\\xi_d}\\right|_{\\xi^{(n_g)}}\\\\\n    \\end{array}\\right]\\end{align}\n\nThe matrix $[\\mathbf{Y}_\\delta]$ is of size $n_g \\times n_d$, where $n_g$ is the number of sampling\npoints and $n_d$ is the number of random variables in the original parameter space. Its SVD is given by:\n\n\\begin{align}\\left[\\mathbf{Y}_\\delta\\right] = \\left[\\mathbf{U}\\right]\\left[\\mathbf{\\Sigma}\\right]\\left[\\mathbf{V}^*\\right]\\end{align}\n\nThe matrix $\\left[\\mathbf{\\Sigma}\\right]$ contains the $n_d$ singular values $\\sigma_i$ of\n$[\\mathbf{Y}_\\delta]$. The projection matrix $[\\mathbf{W}]$ is determined from the right singular\nvectors $[\\mathbf{V}^*]$ by including principal axes up to a limit where the sum of the included singular values reaches\n95% of the total sum of singular values:\n\n\\begin{align}\\sum_{i=1}^{n_d'} \\sigma_i \\leq 0.95\\sum_{i=1}^{n_d} \\sigma_i\\end{align}\n\nHence, the projection matrix $[\\mathbf{W}]$ is given by the first $n_d'$ rows of $[\\mathbf{V}^*]$.\n\n\\begin{align}[\\mathbf{V}^*] =\n    \\left[ \\begin{array}{c}\n    [\\mathbf{W}] \\\\\n    [\\mathbf{W}']\n    \\end{array}\\right]\\end{align}\n\nThe advantage of the SVD approach is that the rotation is optimal in the L2 sense because the new random variables\n$\\eta$ are aligned with the principal axes of the solution. Moreover, the calculation of the SVD of\n$\\left[\\mathbf{Y}_\\delta\\right]$ is fast. The disadvantage of the approach is however, that the gradient of the\nsolution vector is required to determine the projection matrix $[\\mathbf{W}]$. Depending on the chosen\n`gradient calculation approach <label_gradient_calculation_approach>` this may result in additional\nfunction evaluations. Once the gradients have been calculated, however, the gPC coefficients can be computed with higher\naccuracy and less additional sampling points as it is described in the\n`gradient enhanced gPC <label_gradient_enhanced_gpc>`. Accordingly, the choice of which method to select is\n(as usual) highly dependent on the underlying problem and its compression capabilities.\n\nIt is noted that the projection matrix $[\\mathbf{W}]$ has to be determined for\neach QoI separately.\n\nThe projection approach is implemented in the following algorithms:\n\n* `Algorithm: StaticProjection`\n* `Algorithm: RegAdaptiveProjection`\n* `Algorithm: MEStaticProjection`\n* `Algorithm: MERegAdaptiveProjection`\n\n<img src=\"file://examples/images/FD_fwd.png\" width=\"500\" align=\"center\">\n\n## Example\nLets consider the following $n_d$ dimensional testfunction:\n\n\\begin{align}y = \\cos \\left( 2 \\pi u + a\\sum_{i=1}^{n_d}\\xi_i \\right)\\end{align}\n\nwith $u=0.5$ and $a=5.0$. Without loss of generality, we assume the two-dimensional case,\ni.e. $n_d=2$, for now. This function can be expressed by only one random variable $\\eta$, which is a\nlinear combination of the original random variables $\\xi$:\n\n\\begin{align}\\eta = \\sum_{i=1}^{n_d}\\xi_i,\\end{align}\n\nThis function is implemented in the :mod:`testfunctions <pygpc.testfunctions.testfunctions>` module of pygpc in\n:class:`GenzOscillatory <pygpc.testfunctions.testfunctions.GenzOscillatory>`. In the following,\nwe will set up a static gPC with fixed order using the previously described projection approach to reduce the\noriginal dimensionality of the problem.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setting up the problem\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pygpc\nfrom collections import OrderedDict\n\n# Loading the model and defining the problem\n# ------------------------------------------\n\n# Define model\nmodel = pygpc.testfunctions.GenzOscillatory()\n\n# Define problem\nparameters = OrderedDict()\nparameters[\"x1\"] = pygpc.Beta(pdf_shape=[1., 1.], pdf_limits=[0., 1.])\nparameters[\"x2\"] = pygpc.Beta(pdf_shape=[1., 1.], pdf_limits=[0., 1.])\nproblem = pygpc.Problem(model, parameters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setting up the algorithm\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# gPC options\noptions = dict()\noptions[\"method\"] = \"reg\"\noptions[\"solver\"] = \"Moore-Penrose\"\noptions[\"settings\"] = None\noptions[\"interaction_order\"] = 1\noptions[\"n_cpu\"] = 0\noptions[\"error_type\"] = \"nrmsd\"\noptions[\"n_samples_validation\"] = 1e3\noptions[\"error_norm\"] = \"relative\"\noptions[\"matrix_ratio\"] = 2\noptions[\"qoi\"] = 0\noptions[\"fn_results\"] = 'tmp/staticprojection'\noptions[\"save_session_format\"] = \".pkl\"\noptions[\"grid\"] = pygpc.Random\noptions[\"grid_options\"] = {\"seed\": 1}\noptions[\"n_grid\"] = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since we have to compute the gradients of the solution anyway for the projection approach, we will make use of them\nalso when determining the gPC coefficients. Therefore, we enable the \"gradient_enhanced\" gPC. For more details\nplease see `gradient enhanced gPC <label_gradient_enhanced_gpc>`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "options[\"gradient_enhanced\"] = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the following we choose the `method to determine the gradients <label_gradient_calculation_approach>`.\nWe will use a classical first order finite difference forward approximation for now.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "options[\"gradient_calculation\"] = \"FD_fwd\"\noptions[\"gradient_calculation_options\"] = {\"dx\": 0.001, \"distance_weight\": -2}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will use a 10th order approximation. It is noted that the model will consist of only one random variable.\nIncluding the mean (0th order coefficient) there will be 11 gPC coefficients in total.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "options[\"order\"] = [10]\noptions[\"order_max\"] = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we are defining the :class:`StaticProjection <pygpc.Algorithm.StaticProjection>` algorithm to solve the given problem.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "algorithm = pygpc.StaticProjection(problem=problem, options=options)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Running the gpc\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Initialize gPC Session\nsession = pygpc.Session(algorithm=algorithm)\n\n# Run gPC algorithm\nsession, coeffs, results = session.run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inspecting the gpc object\nThe SVD of the gradients of the solution vector resulted in the following projection matrix reducing the problem\nfrom the two dimensional case to the one dimensional case:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(f\"Projection matrix [W]: {session.gpc[0].p_matrix}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It is of size $n_d' \\times n_d$, i.e. $[1 \\times 2]$ in our case. Because of the simple sum of the\nrandom variables it can be seen directly from the projection matrix that the principal axis is 45\u00b0 between the\noriginal parameter axes, exactly as the SVD of the gradient of the solution predicts. As a result,\nthe number of gPC coefficients for a 10th order gPC approximation with only one random variable is 11:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(f\"Number of gPC coefficients: {session.gpc[0].basis.n_basis}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Accordingly, the gPC matrix has 11 columns, one for each gPC coefficient:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(f\"Size of gPC matrix: {session.gpc[0].gpc_matrix.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It was mentioned previously that the one can make use of the\n`gradient enhanced gPC <label_gradient_enhanced_gpc>` when using the projection approach.\nInternally, the gradients are also rotated and the gPC matrix is extended by the gPC matrix\ncontaining the derivatives:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(f\"Size of gPC matrix containing the derivatives: {session.gpc[0].gpc_matrix_gradient.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The random variables of the original and the reduced problem can be reviewed in:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(f\"Original random variables: {session.gpc[0].problem_original.parameters_random}\")\nprint(f\"Reduced random variables: {session.gpc[0].problem.parameters_random}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Postprocessing\nThe post-processing works identical as in a standard gPC. The routines identify whether the problem is reduced\nand provide all sensitivity measures with respect to the original model parameters.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Post-process gPC and save sensitivity coefficients in .hdf5 file\npygpc.get_sensitivities_hdf5(fn_gpc=options[\"fn_results\"],\n                             output_idx=None,\n                             calc_sobol=True,\n                             calc_global_sens=True,\n                             calc_pdf=False,\n                             algorithm=\"sampling\",\n                             n_samples=10000)\n\n# Get summary of sensitivity coefficients\nsobol, gsens = pygpc.get_sens_summary(fn_gpc=options[\"fn_results\"],\n                                      parameters_random=session.parameters_random,\n                                      fn_out=None)\nprint(f\"\\nSobol indices:\")\nprint(f\"==============\")\nprint(sobol)\n\nprint(f\"\\nGlobal average derivatives:\")\nprint(f\"===========================\")\nprint(gsens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Validation\nValidate gPC vs original model function (2D-surface)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nThe possibility of parameter reduction becomes best clear if one visualizes the function values in the parameter\nspace. In this simple example there is almost no difference between the original model (left) and the reduced gPC\n(center).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pygpc.validate_gpc_plot(session=session,\n                        coeffs=coeffs,\n                        random_vars=list(problem.parameters_random.keys()),\n                        n_grid=[51, 51],\n                        output_idx=[0],\n                        fn_out=None,\n                        folder=None,\n                        n_cpu=session.n_cpu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References\n.. [1] Tipireddy, R., & Ghanem, R. (2014). Basis adaptation in homogeneous chaos spaces.\n   Journal of Computational Physics, 259, 304-317.\n\n.. [2] Tsilifis, P., Huan, X., Safta, C., Sargsyan, K., Lacaze, G., Oefelein, J. C., Najm, H. N.,\n   & Ghanem, R. G. (2019). Compressive sensing adaptation for polynomial chaos expansions.\n   Journal of Computational Physics, 380, 29-47.\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}